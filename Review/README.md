<h2 align="left">EvalProReview: A collection of 111 AI4SE benchmarking papers.</h2>

For a more semantic and searchable version of this review you can reffer to our [website](http://evalpro.online)

Additionally, for a better classification of the papers, you can refer to the EvalPro paper where we have organized the papers into a set of 13 tables to help you navigate the papers more easily.

| Publication Year | Title | Author |  Publication Title |
| --- | ---------- | --- | --- |
| 2022 | [Can we generate shellcodes via natural language? An empirical study](https://doi.org/10.1007/s10515-022-00331-3) | Liguori, Pietro,  Pietro Liguori,  Erfan Al-Hossami,  Erfan Al-Hossami,  Domenico Cotroneo, et al. | Automated software engineering |
| 2023 | [TACO: Topics in Algorithmic COde generation dataset](https://doi.org/10.48550/arxiv.2312.14852) | Rongao Li,  Jie Fu,  Bo-Wen Zhang,  Tao Huang,  Zhihong Sun, et al. | arXiv.org |
| 2023 | [Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code](https://doi.org/10.48550/arxiv.2312.14856) | Shahin Honarvar,  Mark Van Der Wilk,  Alastair Donaldson | arXiv.org |
| 2024 | [PECC: Problem Extraction and Coding Challenges](https://doi.org/10.48550/arxiv.2404.18766) | Patrick Haller,  Jonas Golde,  Alan Akbik | International Conference on Language Resources and Evaluation |
| 2024 | [RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale](https://doi.org/10.48550/arxiv.2406.16801) | Beck Labash,  August Rosedale,  Alex Reents,  Lucas Negritto,  Colin Wiel | arXiv.org |
| 2024 | [CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences](https://doi.org/10.48550/arxiv.2403.09032) | M. Weyssow,  Aton Kamanda,  H. Sahraoui | arXiv.org |
| 2024 | [CREF: An LLM-based Conversational Software Repair Framework for Programming Tutors](https://doi.org/10.48550/arxiv.2406.13972) | Boyang Yang,  Haoye Tian,  Weiguo Pian,  Haoran Yu,  Haitao Wang, et al. | arXiv.org |
| 2015 | [The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs](https://doi.org/10.1109/tse.2015.2454513) | Claire Le Goues,  Le Goues, Claire,  Neal J. Holtschulte,  Holtschulte, Neal J.,  E. K. M. Smith, et al. | IEEE Transactions on Software Engineering |
| 2023 | [Can LLM Already Serve as A Database Interface? A BIg Bench for   Large-Scale Database Grounded Text-to-SQLs](https://doi.org/10.48550/arxiv.2305.03111) | Jinyang Li,  Binyuan Hui,  Ge Qu,  Binhua Li,  Jun Yang, et al. | Neural Information Processing Systems |
| 2024 | [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](https://doi.org/10.48550/arXiv.2407.12883) | Hongjin Su,  Howard Yen,  Mengzhou Xia,  Weijia Shi,  Niklas Muennighoff, et al. | nan |
| 2023 | [Predicting Code Coverage without Execution](https://doi.org/10.48550/arxiv.2307.13383) | Michele Tufano,  Shubham Chandel,  Anisha Agarwal,  Neel Sundaresan,  Colin B. Clement | arXiv.org |
| 2024 | [BioCoder: a benchmark for bioinformatics code generation with large language models](https://doi.org/10.1093/bioinformatics/btae230) | Xiangru Tang,  Bill Qian,  Rick Gao,  Jiakang Chen,  Xinyun Chen, et al. | Bioinformatics |
| 2021 | [KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers](https://doi.org/10.18653/v1/2021.acl-long.176) | Chia-Hsuan Lee,  Oleksandr Polozov,  Matthew Richardson | Annual Meeting of the Association for Computational Linguistics |
| 2022 | [SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques](https://doi.org/10.1145/3549035.3561184) | Mohammed Latif Siddiq,  Mohammed Latif Siddiq,  Joanna C. S. Santos,  Joanna C. S. Santos | nan |
| 2024 | [Experimenting a New Programming Practice with LLMs](https://doi.org/10.48550/arxiv.2401.01062) | Simiao Zhang,  Jiaping Wang,  Guoliang Dong,  Jun Sun,  Yueling Zhang, et al. | arXiv.org |
| 2021 | [PSB2: the second program synthesis benchmark suite](https://doi.org/10.1145/3449639.3459285) | Thomas Helmuth,  Helmuth, Thomas,  Peter Kelly,  Kelly, Peter | Annual Conference on Genetic and Evolutionary Computation |
| 2023 | [LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations](https://doi.org/10.1109/msr59073.2023.00084) | Catherine Tony,  Markus Mutas,  Nicolás E. Díaz Ferreyra,  Riccardo Scandariato | IEEE Working Conference on Mining Software Repositories |
| 2024 | [Exploring and Evaluating Hallucinations in LLM-Powered Code Generation](https://doi.org/10.48550/arxiv.2404.00971) | Fang Liu,  Yang Liu,  Lin Shi,  Houkun Huang,  Ruifeng Wang, et al. | arXiv.org |
| 2021 | [CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software.](https://doi.org/10.1145/3475960.3475985) | Guru Prasad Bhandari,  Bhandari, Prasad,  Amara Naseer,  Naseer, Amara,  Leon Moonen, et al. | arXiv: Software Engineering |
| 2020 | [Defining a Software Maintainability Dataset: Collecting, Aggregating and Analysing Expert Evaluations of Software Maintainability](https://doi.org/10.1109/icsme46990.2020.00035) | Markus Schnappinger,  Schnappinger, Markus,  Arnaud Fietzke,  Fietzke, Arnaud,  Alexander Pretschner, et al. | IEEE International Conference on Software Maintenance and Evolution |
| 2022 | [Automating code review activities by large-scale pre-training](https://doi.org/10.1145/3540250.3549081) | Zhiyu Li,  Zhiyu Li,  Lu, Shuai,  Shiqiang Lu,  Guo, Daya, et al. | ESEC/SIGSOFT FSE |
| 2024 | [A Survey on Large Language Models for Code Generation](https://doi.org/10.48550/arxiv.2406.00515) | Juyong Jiang,  Fan Wang,  Jiasi Shen,  Sungju Kim,  Sunghun Kim | arXiv.org |
| 2024 | [CodeS: Natural Language to Code Repository via Multi-Layer Sketch](https://doi.org/10.48550/arxiv.2403.16443) | Daoguang Zan,  Ailun Yu,  Wei Liu,  Dong Chen,  Bo Shen, et al. | arXiv.org |
| 2023 | [RepoFusion: Training Code Models to Understand Your Repository](https://doi.org/10.48550/arxiv.2306.10998) | Disha Shrivastava,  Denis Kocetkov,  Harm de Vries,  Dzmitry Bahdanau,  Torsten Scholak | arXiv.org |
| 2022 | [Execution-based Evaluation for Data Science Code Generation Models](https://doi.org/10.48550/arxiv.2211.09374) | Huang, Junjie,  Junjie Huang,  Wang, Chenglong,  Chenglong Wang,  Zhang, Jipeng, et al. | Cornell University - arXiv |
| 2023 | [StudentEval: A Benchmark of Student-Written Prompts for Large Language   Models of Code](https://doi.org/10.48550/arxiv.2306.04556) | Hannah McLean Babe,  Sean Nguyen,  Yangtian Zi,  Arjun Guha,  Molly Q Feldman, et al. | arXiv.org |
| 2022 | [Execution-Based Evaluation for Open-Domain Code Generation](https://doi.org/10.48550/arxiv.2212.10481) | Zhiruo Wang,  Shao-Lai Zhou,  Daniel Fried,  Graham Neubig | Conference on Empirical Methods in Natural Language Processing |
| 2023 | [A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends](https://doi.org/10.48550/arxiv.2311.10372) | Zibin Zheng,  Kaiwen Ning,  Yanlin Wang,  Jingwen Zhang,  Dewu Zheng, et al. | arXiv.org |
| 2023 | [A Critical Review of Large Language Model on Software Engineering: An   Example from ChatGPT and Automated Program Repair](https://doi.org/10.48550/arxiv.2310.08879) | Quanjun Zhang,  Tongke Zhang,  Jun Zhai,  Chunrong Fang,  Bo Yu, et al. | arXiv.org |
| 2017 | [QuixBugs: a multi-lingual program repair benchmark set based on the quixey challenge](https://doi.org/10.1145/3135932.3135941) | Derrick Lin,  Lin, Derrick,  James Koppel,  Koppel, James,  Angela Chen, et al. | ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity |
| 2024 | [DebugBench: Evaluating Debugging Capability of Large Language Models](https://doi.org/10.48550/arxiv.2401.04621) | Runchu Tian,  Yining Ye,  Yujia Qin,  Xin Cong,  Yankai Lin, et al. | arXiv.org |
| 2023 | [A Survey on Large Language Models for Software Engineering](https://doi.org/10.48550/arxiv.2312.15223) | Quanjun Zhang,  Chunrong Fang,  Yang Xie,  Yaxin Zhang,  Yun Yang, et al. | arXiv.org |
| 2023 | [CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models](https://doi.org/10.1109/icse48619.2023.00055) | Changan Niu,  Chuanyi Li,  Vincent Ng,  Bin Luo | International Conference on Software Engineering |
| 2024 | [CRQBench: A Benchmark of Code Reasoning Questions](https://doi.org/10.48550/arXiv.2408.08453) | Elizabeth Dinella,  Satish Chandra,  Petros Maniatis | nan |
| 2023 | [The GitHub Recent Bugs Dataset for Evaluating LLM-based Debugging   Applications](https://doi.org/10.48550/arxiv.2310.13229) | Jae Yong Lee,  Shichang Kang,  Juyeon Yoon,  Shin Yoo | arXiv.org |
| 2024 | [The Emergence of Large Language Models in Static Analysis: A First Look Through Micro-Benchmarks](https://doi.org/10.1145/3650105.3652288) | Ashwini Venkatesh,  Samkutty Sabu,  Amir M. Mir,  Sofia Reis,  Eric Bodden | 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym: |
| 2023 | [TypeEvalPy: A Micro-Benchmarking Framework for Python Type Inference Tools](https://doi.org/10.1145/3639478.3640033) | Ashwin Prasad Shivarpatna Venkatesh,  Samkutty Sabu,  Jiawei Wang,  Amir M. Mir,  Li Li, et al. | 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion) |
| 2015 | [Suggesting accurate method and class names](https://doi.org/10.1145/2786805.2786849) | Miltiadis Allamanis,  Allamanis, Miltiadis,  Earl T. Barr,  Barr, Earl T.,  Christian Bird, et al. | ESEC/SIGSOFT FSE |
| 2016 | [A Convolutional Attention Network for Extreme Summarization of Source Code](https://doi.org/10.48550/arXiv.1602.03001) | Miltiadis Allamanis,  Allamanis, Miltiadis,  Hao Peng,  Peng, Hao,  Charles Sutton, et al. | arXiv: Learning |
| 2019 | [A neural model for generating natural language summaries of program subroutines](https://doi.org/10.1109/icse.2019.00087) | Alexander LeClair,  LeClair, Alexander,  Siyuan Jiang,  Jiang, Siyuan,  Collin McMillan, et al. | International Conference on Software Engineering |
| 2018 | [StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow](https://doi.org/10.1145/3178876.3186081) | Zhiliang Yao,  Yao, Ziyu,  Daniel S. Weld,  Weld, D.,  Wei-Peng Chen, et al. | The Web Conference |
| 2024 | [GitBug-Java: A Reproducible Benchmark of Recent Java Bugs](https://doi.org/10.1145/3643991.3644884) | Andr'e Silva,  Nuno Saavedra,  Martin Monperrus | IEEE Working Conference on Mining Software Repositories |
| 2024 | [CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution](https://doi.org/10.48550/arxiv.2401.03065) | Alex Gu,  Baptiste Rozière,  Hugh Leather,  Armando Solar-Lezama,  Gabriel Synnaeve, et al. | arXiv.org |
| 2024 | [ConflictBench: A benchmark to evaluate software merge tools](https://doi.org/10.1016/j.jss.2024.112084) | Bowen Shen,  Na Meng | Journal of Systems and Software |
| 2020 | [Unit Test Case Generation with Transformers](https://doi.org/10.48550/arXiv.2009.05617) | Michele Tufano,  Tufano, Michele,  Dawn Drain,  Drain, Dawn,  A. Svyatkovskiy, et al. | arXiv: Software Engineering |
| 2014 | [Defects4J: a database of existing faults to enable controlled testing studies for Java programs](https://doi.org/10.1145/2610384.2628055) | René Just,  Just, René,  Darioush Jalali,  Jalali, Darioush,  M. Ernst, et al. | International Symposium on Software Testing and Analysis |
| 2018 | [NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System](https://doi.org/arXiv.1802.08979) | Xi Victoria Lin,  Lin, Xi Victoria,  Chenglong Wang,  Wang, Chenglong,  Luke Zettlemoyer, et al. | arXiv: Computation and Language |
| 2018 | [Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task](https://doi.org/10.18653/v1/d18-1425) | Tao Yu,  Yu, Tao,  Rui Zhang,  Rui Zhang,  Zhang, Rui, et al. | Conference on Empirical Methods in Natural Language Processing |
| 2017 | [Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning](https://doi.org/10.48550/arXiv.1709.00103) | Victor Zhong,  Zhong, Victor,  Caiming Xiong,  Xiong, Caiming,  Richard Socher, et al. | arXiv: Computation and Language |
| 2015 | [Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)](https://doi.org/10.1109/ase.2015.36) | Yusuke Oda,  Oda, Yusuke,  Hiroyuki Fudaba,  Fudaba, Hiroyuki,  Graham Neubig, et al. | International Conference on Automated Software Engineering |
| 2018 | [NAPS: Natural Program Synthesis Dataset.](https://doi.org/10.48550/arXiv.1807.03168) | Maksym Zavershynskyi,  Maksym Zavershynskyi,  Zavershynskyi, Maksym,  Alexander Skidanov,  Skidanov, Alexander, et al. | arXiv: Learning |
| 2019 | [SPoC: Search-based Pseudocode to Code](https://doi.org/10.48550/arXiv.1906.04908) | Sumith Kulal,  Kulal, Sumith,  Panupong Pasupat,  Pasupat, Panupong,  Kartik Chandra, et al. | arXiv: Learning |
| 2024 | [CodeRAG-Bench: Can Retrieval Augment Code Generation?](https://doi.org/10.48550/arxiv.2406.14497) | Zora Zhiruo Wang,  Akari Asai,  Xinyan Velocity Yu,  Frank F. Xu,  Yiqing Xie, et al. | arXiv.org |
| 2023 | [API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.187) | Minghao Li,  Feifan Song,  Yu Bowen,  Haiyang Yu,  Zhoujun Li, et al. | Conference on Empirical Methods in Natural Language Processing |
| 2023 | [Gorilla: Large Language Model Connected with Massive APIs](https://doi.org/10.48550/arxiv.2305.15334) | Shishir G. Patil,  Tianjun Zhang,  Xin Wang,  Joseph E. Gonzalez | arXiv.org |
| 2018 | [API method recommendation without worrying about the task-API knowledge gap](https://doi.org/10.1145/3238147.3238191) | Qiao Huang,  Huang, Qiao,  Xin Xia,  Xia, Xin,  Zhenchang Xing, et al. | International Conference on Automated Software Engineering |
| 2021 | [Revisiting, Benchmarking and Exploring API Recommendation: How Far Are We?](https://doi.org/10.1109/tse.2022.3197063) | Yun Peng,  Shuqing Li,  Wenwei Gu,  Yichen Li,  Wenxuan Wang, et al. | IEEE Transactions on Software Engineering |
| 2023 | [RestGPT: Connecting Large Language Models with Real-World RESTful APIs](https://doi.org/10.48550/arXiv.2306.06624) | Yifan Song,  Weimin Xiong,  Dawei Zhu,  Wenhao Wu,  Han Qian, et al. | nan |
| 2024 | [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](https://doi.org/10.48550/arxiv.2405.19856) | Jia Li,  Ge Li,  Yunfei Zhao,  Yongming Li,  Huanyu Liu, et al. | arXiv.org |
| 2024 | [DevBench: A Comprehensive Benchmark for Software Development](https://doi.org/10.48550/arxiv.2403.08604) | Bowen Li,  Wenhan Wu,  Ziwei Tang,  Lin Shi,  John Yang, et al. | arXiv.org |
| 2024 | [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](https://doi.org/10.48550/arxiv.2406.15877) | Terry Yue Zhuo,  Minh Chien Vu,  Jenny Chim,  Han Hu,  Wenhao Yu, et al. | arXiv.org |
| 2024 | [EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories](https://doi.org/10.48550/arxiv.2404.00599) | Jia Li,  Ge Li,  Xuanming Zhang,  Yihong Dong,  Zhi Jin | arXiv.org |
| 2024 | [CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios](https://doi.org/10.48550/arxiv.2403.19287) | Zhengran Zeng,  Yidong Wang,  Rui Xie,  Wei Ye,  Shi-Bo Zhang | arXiv.org |
| 2023 | [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval   and Generation](https://doi.org/10.48550/arxiv.2303.12570) | Fengji Zhang,  Bei Chen,  Yue Zhang,  Jin Liu,  Daoguang Zan, et al. | Conference on Empirical Methods in Natural Language Processing |
| 2023 | [RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems](https://doi.org/10.48550/arxiv.2306.03091) | Tianyang Liu,  Canwen Xu,  Julian McAuley | International Conference on Learning Representations |
| 2023 | [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://doi.org/10.48550/arxiv.2310.06770) | Carlos E. Jiménez,  John Yang,  Alexander Wettig,  Shunyu Yao,  Kexin Pei, et al. | International Conference on Learning Representations |
| 2023 | [CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code   Completion](https://doi.org/10.48550/arxiv.2310.11248) | Yangruibo Ding,  Zijian Wang,  Wasi Uddin Ahmad,  Hantian Ding,  Ming Tan, et al. | Neural Information Processing Systems |
| 2023 | [ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on   Class-level Code Generation](https://doi.org/10.48550/arxiv.2308.01861) | Xueying Du,  Mingwei Liu,  Qing Wang,  Hanlin Wang,  Junwei Liu, et al. | arXiv.org |
| 2023 | [AVATAR: A Parallel Corpus for Java-Python Program Translation](https://doi.org/10.18653/v1/2023.findings-acl.143) | Wasi Uddin Ahmad,  Md. Rafat Rahman Tushar,  Saikat Chakraborty,  Kai-Wei Chang | Annual Meeting of the Association for Computational Linguistics |
| 2023 | [CodeTransOcean: A Comprehensive Multilingual Benchmark for Code   Translation](https://doi.org/10.48550/arxiv.2310.04951) | Weixiang Yan,  Ye Tian,  Yunzhe Li,  Qian Chen,  Wen Wang | Conference on Empirical Methods in Natural Language Processing |
| 2022 | [Multilingual Code Snippets Training for Program Translation](https://doi.org/10.1609/aaai.v36i10.21434) | Ming Zhu,  Ming Zhu,  K. Suresh,  Karthik Suresh,  Chandan K. Reddy, et al. | Proceedings of the ... AAAI Conference on Artificial Intelligence |
| 2021 | [Leveraging Automated Unit Tests for Unsupervised Code Translation](https://doi.org/10.48550/arXiv.2110.06773) | Baptiste Rozière,  J Zhang,  François Charton,  M. Harman,  Gabriel Synnaeve, et al. | International Conference on Learning Representations |
| 2024 | [Long Code Arena: a Set of Benchmarks for Long-Context Code Models](https://doi.org/10.48550/arxiv.2406.11612) | Egor Bogomolov,  Aleksandra Eliseeva,  Timur Galimzyanov,  Evgeniy Glukhov,  Anton Shapkin, et al. | arXiv.org |
| 2023 | [Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models](https://doi.org/10.48550/arxiv.2312.09601) | Xin Jin,  Jonathan Larson,  Weiwei Yang,  Zhiqiang Lin | arXiv.org |
| 2018 | [Deep code comment generation](https://doi.org/10.1145/3196321.3196334) | Xing Hu,  Hu, Xing,  Ge Li,  Li, Ge,  Xin Xia, et al. | IEEE International Conference on Program Comprehension |
| 2019 | [CodeSearchNet Challenge: Evaluating the State of Semantic Code Search.](https://doi.org/10.48550/arXiv.1909.09436) | Hamel Husain,  Husain, Hamel,  Ho-Hsiang Wu,  Ho-Hsiang Wu,  Ho-Hsiang Wu, et al. | arXiv: Learning |
| 2024 | [InfiCoder-Eval: Systematically Evaluating the Question-Answering Capabilities of Code Large Language Models](https://doi.org/10.48550/arxiv.2404.07940) | Linyi Li,  Shijie Geng,  Zhenwen Li,  Yibo He,  Hao Yu, et al. | arXiv.org |
| 2022 | [XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence](https://doi.org/10.48550/arxiv.2206.08474) | Ming Zhu,  Aneesh Jain,  Karthik Suresh,  Roshan Ravindran,  Sindhu Tipirneni, et al. | arXiv.org |
| 2023 | [xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code   Understanding, Generation, Translation and Retrieval](https://doi.org/10.48550/arxiv.2303.03004) | Mohammad Abdullah Matin Khan,  M Saiful Bari,  Xuan Long Do,  Weishi Wang,  Md Rizwan Parvez, et al. | arXiv.org |
| 2022 | [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://doi.org/10.48550/arXiv.2203.13474) | Erik Nijkamp,  Bo Pang,  Hiroaki Hayashi,  Lifu Tu,  Haiquan Wang, et al. | International Conference on Learning Representations |
| 2018 | [Mapping Language to Code in Programmatic Context](https://doi.org/10.18653/v1/d18-1192) | Srinivasan Iyer,  Iyer, Srinivasan,  Ioannis Konstas,  Konstas, Ioannis,  Alvin Cheung, et al. | arXiv: Computation and Language |
| 2021 | [CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](https://doi.org/arXiv.2102.04664) | Shuai Lu,  Daya Guo,  Shuo Ren,  Junjie Huang,  Alexey Svyatkovskiy, et al. | NeurIPS Datasets and Benchmarks |
| 2021 | [CoSQA: 20,000+ Web Queries for Code Search and Question Answering](https://doi.org/10.18653/v1/2021.acl-long.442) | Junjie Huang,  Duyu Tang,  Linjun Shou,  Ming Gong,  Ke Xu, et al. | Annual Meeting of the Association for Computational Linguistics |
| 2022 | [AixBench: A Code Generation Benchmark Dataset](https://doi.org/10.48550/arxiv.2206.13179) | Yiyang Hao,  Ge Li,  Yongqiang Liu,  Xiaowei Miao,  He Zong, et al. | arXiv.org |
| 2018 | [Learning to mine aligned code and natural language pairs from stack overflow](https://doi.org/10.1145/3196398.3196408) | Pengcheng Yin,  Yin, Pengcheng,  Bowen Deng,  Deng, Bowen,  Edgar Chen, et al. | IEEE Working Conference on Mining Software Repositories |
| 2022 | [MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages](https://doi.org/10.48550/arxiv.2203.08388) | Zhiruo Wang,  Grace Cuenca,  Shuyan Zhou,  Frank F. Xu,  Graham Neubig | Findings |
| 2023 | [TheoremQA: A Theorem-driven Question Answering dataset](https://doi.org/10.48550/arxiv.2305.12524) | Wenhu Chen,  Min Yin,  Mong-Kai Ku,  Eric A. Wan,  Xueguang Ma, et al. | Conference on Empirical Methods in Natural Language Processing |
| 2022 | [PAL: Program-aided Language Models](https://doi.org/10.48550/arxiv.2211.10435) | Gao, Luyu,  Luyu Gao,  Madaan, Aman,  Aman Madaan,  Zhou, Shuyan, et al. | Cornell University - arXiv |
| 2015 | [Solving General Arithmetic Word Problems](https://doi.org/10.18653/v1/d15-1202) | Subhro Roy,  Roy, Subhro,  Dan Roth,  Roth, Dan | Conference on Empirical Methods in Natural Language Processing |
| 2022 | [Lila: A Unified Benchmark for Mathematical Reasoning](https://doi.org/10.48550/arxiv.2210.17517) | Mishra, Swaroop,  Swaroop Mishra,  Finlayson, Matthew,  Matthew Finlayson,  Lu, Pan, et al. | Cornell University - arXiv |
| 2019 | [MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms.](https://doi.org/10.18653/v1/n19-1245) | Aida Amini,  Amini, Aida,  Saadia Gabriel,  Gabriel, Saadia,  Peter Lin, et al. | arXiv: Computation and Language |
| 2022 | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://doi.org/10.48550/arXiv.2206.04615) | Aarohi Srivastava,  Abhinav Rastogi,  Abhishek Rao,  Abu Awal Md Shoeb,  Abubakar Abid, et al. | arXiv.org |
| 2022 | [Training and Evaluating a Jupyter Notebook Data Science Assistant](https://doi.org/10.48550/arXiv.2201.12901) | Shubham Chandel,  Colin B. Clement,  Guillermo Serrato,  Neel Sundaresan | arXiv.org |
| 2019 | [JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation](https://doi.org/10.18653/v1/d19-1546) | Rajas Agashe,  Agashe, Rajas,  Srinivasan Iyer,  Iyer, Srinivasan,  Luke Zettlemoyer, et al. | arXiv: Learning |
| 2022 | [CERT: Continual Pre-Training on Sketches for Library-Oriented Code   Generation](https://doi.org/10.48550/arxiv.2206.06888) | Zan, Daoguang,  Daoguang Zan,  Chen, Bei,  Bei Chen,  Yang, Dejian, et al. | Cornell University - arXiv |
| 2024 | [PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs](https://doi.org/10.48550/arXiv.2401.03855) | Ankit Yadav,  Himanshu Beniwal,  Mayank Singh | nan |
| 2024 | [EffiBench: Benchmarking the Efficiency of Automatically Generated Code](https://doi.org/10.48550/arxiv.2402.02037) | Dong Huang,  Jie M. Zhang,  Yuhao Qing,  Heming Cui | arXiv.org |
| 2024 | [CodeComplex: A Time-Complexity Dataset for Bilingual Source Codes](https://doi.org/10.48550/arxiv.2401.08719) | Seung-Yeop Baik,  Mingi Jeon,  Joonghyuk Hahn,  Jungin Kim,  Yo-Sub Han, et al. | arXiv.org |
| 2023 | [TASTY: A Transformer based Approach to Space and Time complexity](https://doi.org/10.48550/arxiv.2305.05379) | Kaushik Moudgalya,  A. G. Ramakrishnan,  Vamsikrishna Chemudupati,  Xing Lu | arXiv.org |
| 2019 | [Learning based Methods for Code Runtime Complexity Prediction.](https://doi.org/10.1007/978-3-030-45439-5_21) | Jagriti Sikka,  Sikka, Jagriti,  Kushal Satya,  Satya, Kushal,  Yaman Kumar, et al. | arXiv: Learning |
| 2021 | [Measuring Coding Challenge Competence With APPS](https://doi.org/10.48550/arXiv.2105.09938) | Dan Hendrycks,  Steven Basart,  Saurav Kadavath,  Mantas Mazeika,  Akul Arora, et al. | NeurIPS Datasets and Benchmarks |
| 2022 | [Competition-level code generation with AlphaCode](https://doi.org/10.1126/science.abq1158) | Yujia Li,  David Choi,  Jun‐Young Chung,  Nate Kushman,  Julian Schrittwieser, et al. | Science |
| 2021 | [Program Synthesis with Large Language Models](https://doi.org/10.48550/arXiv.2108.07732) | Jacob Austin,  Augustus Odena,  Maxwell Nye,  Maarten Bosma,  H. Michalewski, et al. | arXiv.org |
| 2024 | [Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM](https://doi.org/10.48550/arxiv.2403.19114) | Chun Xia,  Yinlin Deng,  Lingming Zhang | arXiv.org |
| 2023 | [CodeScore: Evaluating Code Generation by Learning Code Execution](https://doi.org/10.48550/arxiv.2301.09043) | Yihong Dong,  Jiazheng Ding,  Jian Xue,  Zhuo Li,  Ge Li, et al. | arXiv.org |
| 2023 | [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of   Large Language Models for Code Generation](https://doi.org/10.48550/arxiv.2305.01210) | Jiawei Liu,  Chunqiu Steven Xia,  Yuyao Wang,  Lingming Zhang | Neural Information Processing Systems |
| 2022 | [Multi-lingual Evaluation of Code Generation Models](https://doi.org/10.48550/arxiv.2210.14868) | Athiwaratkun, Ben,  Ben Athiwaratkun,  Gouda, Sanjay Krishna,  Sanjay Krishna Gouda,  Wang, Zijian, et al. | Cornell University - arXiv |
| 2023 | [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X](https://doi.org/10.1145/3580305.3599790) | Qinkai Zheng,  Xin Xiao,  Xu Zou,  Yuxiao Dong,  Shan Wang, et al. | Knowledge Discovery and Data Mining |
| 2023 | [OctoPack: Instruction Tuning Code Large Language Models](https://doi.org/10.48550/arxiv.2308.07124) | Niklas Muennighoff,  Qian Liu,  Armel Zebaze,  Qinkai Zheng,  Binyuan Hui, et al. | International Conference on Learning Representations |
| 2022 | [MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation](https://doi.org/10.48550/arXiv.2208.08227) | Federico Cassano,  John Gouwar,  Daniel Nguyen,  S. Nguyen,  Luna Phipps-Costin, et al. | nan |
| 2021 | [Evaluating Large Language Models Trained on Code](https://doi.org/10.48550/arXiv.2107.03374) | Mark Chen,  Jerry Tworek,  Heewoo Jun,  Qiming Yuan,  Henrique Pondé, et al. | arXiv.org |
